{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sqlite3\n",
    "import random\n",
    "import pyttsx3\n",
    "from threading import Thread\n",
    "from sklearn.utils import shuffle\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the camera space \n",
    "def build_squares():\n",
    "    x,y,w,h = 420,140,10,10 #Dimension of square\n",
    "    d = 10 #\n",
    "    imgCrop = None #Intialize the cropping of image\n",
    "    for i in range(10): #\n",
    "        for j in range(5): #\n",
    "            if np.any(imgCrop == None): #if any imgcrop is empty do nothing continue\n",
    "                imgCrop = img[y:y+h,x:x+w] #new image it capture in the range specified \n",
    "            else:\n",
    "                imgCrop = np.vstack((imgCrop,img[y:y+h,x:x+w])) #Take a sequence of arrays and stack them vertically to make a single array\n",
    "                #for more information https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.vstack.html\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "            #rectangle formed of the specified dimension and draw a green rectangle at the top-right corner of image and 2 is thickness.\n",
    "            x += w+d \n",
    "        x = 420\n",
    "        y += h+d\n",
    "    return imgCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hand_hist():\n",
    "    cam = cv2.VideoCapture(1) #Capture of image from index 1\n",
    "    if cam.read()[0]==False:\n",
    "        cam = cv2.VideoCapture(0) #capture of image from index 0\n",
    "    x,y,w,h = 300,100,300,300 #Dimension of image specified\n",
    "    flagPressedC,flagPressedS = False,False   #Intialize the keyboard S,C\n",
    "    imgCrop = None #Intialize the Image Crop\n",
    "    while True:\n",
    "        img = cam.read()[1] #image read from index 1\n",
    "        img = cv2.flip(img,1) #flip the image horizontal\n",
    "        hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) #convert the image from Blue,Green and Red to Hue,Saturation &Violet\n",
    "        \n",
    "        keypress = cv2.waitkey(1) #Each frame wait for 1 sec and then new frame starts\n",
    "        if keypress == ord('c'):   #It converts the key pressed into integer\n",
    "            hsvCrop = cv2.cvtColor(imgCrop,cv2.COLOR_BGR2HSV) #change the color into BGR2HSV\n",
    "            flagPressedC = True #if the key pressed C\n",
    "            hist = cv2.calcHist([hsvCrop],[0,1],None,[180,256],[0,180,0,256]) #calulate histogram \n",
    "            cv2.normalize(hist,hist,0,255,cv2.NORM_MINMAX) #normalize the images using min-max \n",
    "        elif keypress == ord('s'): #convert 'S' into numerical\n",
    "            flagPressedS = True #if the button s is pressed\n",
    "            break\n",
    "        if flagPressedC: #if keyword 'c' is pressed\n",
    "            dst = cv2.calcBackProject([hsv],[0,1],hist,[0,180,256],1) # It is used for image segmentation or finding objects of interest in an image.\n",
    "            disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))\n",
    "            cv2.filter2D(dst,-1,disc,dst)\n",
    "            blur = cv2.GaussianBlur(dst,(11,11),0)#Bluring the image using GaussianBlur \n",
    "            blur = cv2.medianBlur(blur,15) #Blurring the image using median Blur\n",
    "            ret,thresh = cv2.thershold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) #Set threshold to filter out specified image type\n",
    "            thresh = cv2.merge(thresh,thresh,thresh) #merging the different images into one\n",
    "            res = cv2.bitwise_and(img,thresh) #Calculates the per-element bit-wise conjunction of two arrays or an array and a scalar.\n",
    "            cv2.imshow(thresh,thresh) #show the thershold image\n",
    "        if not flagPressedS: #if the button 'S' is pressed\n",
    "            img_crop = build_squares(img) #First run the function to capture image \n",
    "        cv2.imshow(\"Set hand histogram\",img) #Show the image captured\n",
    "    cam.release() #Release the capture \n",
    "    cv2.destroyAllWindows() #close the windows\n",
    "    with open(\"hist\",\"wb\") as f:\n",
    "        pickle.dumb(hist,f) #save the image dumb\n",
    "        \n",
    "get_hand_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_x,image_y = 50,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hand_hist():\n",
    "    with open('hist','rb') as f:\n",
    "        hist = pickle.load(f)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_create_folder_database():\n",
    "    if not os.path.exists(\"gestures\"):\n",
    "        os.mkdir(\"gestures\")\n",
    "    if not os.path.exists(\"guesture_db.db\"):\n",
    "        conn = sqlite3.connect(\"gesture_db.db\")\n",
    "        create_table_cmd = \"CREATE TABLE gesture ( g_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE, g_name TEXT NOT NULL )\"\n",
    "        conn.execute(create_table_cmd)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_empty_images():\n",
    "    create_folder(\"guestures/\"+ folder_name)\n",
    "    black = np.zeros(shape = (image_x,image_y,1),dtype=np.uint8)\n",
    "    for i in ranges(n_images):\n",
    "        cv2.imwrite('guestures/'+folder_name+'/'+str(i+1)+'.jpg',black) \n",
    "#save images in black and white in the folder and with name start 1 to till last and save in .jpg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_in_db(g_id,g_name):\n",
    "    conn = sqlite3.connect(\"gesture_db.db\") #connect with sqlite database we have created above bloack\n",
    "    cmd = \"INSERT INTO gesture (g_id, g_name) VALUES (%s, \\'%s\\')\" % (g_id, g_name) #insert the g_id and g_name into sqlite database \n",
    "    try:\n",
    "        conn.execute(cmd) #trying the above to connect using the try function\n",
    "    except sqlite3.IntrgrityError: #except the following error\n",
    "            choice = input(\"g_id already exists. Want to change the record? (y/n): \")\n",
    "            if choice.lower == 'y':\n",
    "                cmd = \"UPDATE gesture SET g_name = \\'%s\\' WHERE g_id = %s\" % (g_name, g_id)\n",
    "                conn.execute(cmd)\n",
    "            else:\n",
    "                print('Doing nothing.....')\n",
    "                return\n",
    "#store image in sqlite database by coonecting the sqlite database where it store image id and image name using try and\n",
    "#exception function if image exists in database then it will ask to upddate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_images(g_id):\n",
    "    total_pics = 1200 #total number of pictures going to save\n",
    "    if g_id == str(0): #if iamge_id is zero\n",
    "        create_empty_images(\"0\",total_pics) #then create number of empty images from zero to 1200\n",
    "    hist = get_hand_hist() #**get history of hand I guess it's MNIST data avaliable in kaggle\n",
    "    #For more information https://www.kaggle.com/c/digit-recognizer/data\n",
    "    cam = cv2.VideoCapture(1) #Used opencv library to capture image\n",
    "    #To capture a video, you need to create a VideoCapture object. Its argument can be either the device index or\n",
    "    #the name of a video file. Device index is just the number to specify which camera. Normally one camera\n",
    "    #will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing\n",
    "    #1 and so on\n",
    "    if cam.read()[0] == False: #if camera doesnot read from the previous camera index then capture camera index 0\n",
    "        cam = cv2.VideoCapture(0)\n",
    "    x,y,w,h = 300,100,300,300  # it's length&width of image x,y,w,h \n",
    "    create_folder(\"gestures\"+str(g_id)) # create a folder gestures and save all the files \n",
    "    pic_no = 0 #Initialise the pic_no variable\n",
    "    flag_start_capturing = False #Initialise frame caputing image\n",
    "    frames = 0 #initalise frame\n",
    "    while True: #when it read the image\n",
    "        img = cam.read()[1] #image capture by videocapture read and store in Img\n",
    "        img = cv2.flip(img,1) #flip every image in horizontal direction '1' nd '0' in vertical images\n",
    "        imgHSV = cv2.ctvColor(img,cv2.COLOR_BGR2HSV) #Convert (BGR) Blue,Green and Red into (HSV) Hue,Saturation and Chroma\n",
    "        #For more information https://stackoverflow.com/questions/17239253/opencv-bgr2hsv-creates-lots-of-artifacts\n",
    "        dst = cv2.calcBackProject([imgHSV],[0,1],hist,[0,180,0,256],1) #Calculates the back projection of a histogram.\n",
    "        #For more information https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=calcbackproject\n",
    "        disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10)) #For elliptical/circular shaped kernels shape.\n",
    "        #For more information https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_morphological_ops/py_morphological_ops.html\n",
    "        cv2.filter2D(dst,-1,disc,dst) # we are try an averaging filter on an image\n",
    "        #For more information https://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html\n",
    "        blur = cv2.GaussianBlur(dst,(11,11),0) #to blurr image\n",
    "        #For more information https://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html\n",
    "        #To make the image noise free \n",
    "        blur = cv2.medianBlur(blur,15)# to blurr image by another method\n",
    "        #takes median of all the pixels under kernel area and central element is replaced with this median value\n",
    "        #Another method to make the image noise free\n",
    "        #For more information https://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html\n",
    "        thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1] #thershold to define filter out the images of grey scale image\n",
    "        #For more information https://docs.opencv.org/3.4.0/d7/d4d/tutorial_py_thresholding.html\n",
    "        thresh = cv2.merge(thresh,thresh,thresh) # Merging of images \n",
    "        #For more information https://docs.opencv.org/3.1.0/d3/df2/tutorial_py_basic_ops.html\n",
    "        thresh = cv2.cvtColor(thresh,cv2.COLOR_BGR2GRAY) #converting image into GRAY\n",
    "        contours = cv2.findContours(trhresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)[1] #a curve joining all the continuous points (along the boundary), having same color or intensity.\n",
    "        #For more information https://docs.opencv.org/3.1.0/d4/d73/tutorial_py_contours_begin.html\n",
    "        if len(contours)>0: #if length(contours greater than zero)\n",
    "            contour = max(contours,key = cv2.contourArea)# maximum between contours and contourArea\n",
    "            if cv2.contourArea(contour)>10000 and  frames>50: #number of frames greater than 50 and ContourArea greater than 1000\n",
    "                x1,y1,w1,h1 = cv2.boundingRect(contour) #convert the image into rectange \n",
    "                pic_no += 1 #calculate number of image\n",
    "                save_img = thresh[y1:y1+h1,x1:x1+w1] #save image by filtering threshold and define the width and length\n",
    "                if w1 > h1: #if width is greater than height then copy border below formula\n",
    "                    save_img = cv2.copyMakeBorder(save_img,int((w1-h1)/2),int((w1-h1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "                elif h1 > w1: #if height is greater than width then copyborder below formula\n",
    "                    save_img = cv2.copyMakeBorder(save_img,int((h1-w1)/2),int((h1-w1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "                save_img = cv2.resize(save_img,(image_x,image_y)) #save entire  image above specified dimension \n",
    "                cv2.putText(img,'Capturing....',(30,60),cv2.FONT_HERSHEY_TRIPLEX,2,(127,255,255))#putText renders the specified text string in the image.\n",
    "                #For more information https://docs.opencv.org/3.1.0/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576\n",
    "                cv2.imwrite(\"guesture/\"+str(g_id)+\"/\"+str(pic_no)+\".jpg\",save_img) #saves the image to the specified file. \n",
    "                #For more information https://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html#bool imwrite(const String& filename, InputArray img, const vector<int>& params)                \n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) #Save images into rectangle specified heigh and width and specified image pixek \n",
    "        cv2.putText(img,str(pic_no),(30,400),cv2.FONT_HERSHEY_TRIPLEX,1.5,(127,127,255))\n",
    "        cv2.imshow(\"Capturing guesture\",img) #show the images captured\n",
    "        cv2.imshow(\"thresh\",thresh) #show the images after filtering out from thershold \n",
    "        keypress = cv2.waitkey(1) #displays the image for specified milliseconds\n",
    "        #For example, waitKey(0) will display the window infinitely until any keypress (it is suitable for\n",
    "        #image display). waitKey(25) will display a frame for 25 ms, after which display will be automatically\n",
    "        #closed. (If you put it in a loop to read videos, it will display the video frame-by-frame)\n",
    "        if keypress == ord('c'): #Given a string of length one, return an integer representing the Unicode code point\n",
    "            #of the character when the argument is a unicode object, or the value of the byte when the argument is an \n",
    "            #8-bit string\n",
    "            if flag_start_capturing == False:\n",
    "                #if camera is not capturing image then start capture image\n",
    "                flag_start_capturing = True\n",
    "            else:\n",
    "                flag_start_capturing = False \n",
    "                #if its's not capturing the image\n",
    "                frames = 0  #then number of frames is zero\n",
    "        if flag_start_captring == True: #if flag start capturing then calculate number of frames and count number of total pictures \n",
    "            frame += 1\n",
    "        if pic_no == total_pics: #if total picture equivalent to define total pictures then break coe out of loop\n",
    "            break\n",
    "init_create_folder_database() # intialise the function\n",
    "g_id = input(\"Enter gesture no.:\") # input gesture no by showing hand \n",
    "g_name  = input(\"Enter gesture name/text:\") #Input gesture name  \n",
    "store_in_db(g_id,g_name) #store image into sqlite database\n",
    "store_images(g_id) #store image in the folder satisfying the specified condition in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Labelling the image \n",
    "def pickle_images_labels():\n",
    "    gest_folder = \"gestures\" #folder where image are saved\n",
    "    images_labels = [] #Intialize the array Labeled image \n",
    "    images = [] #intialize images array\n",
    "    labels = [] #intialize labels array\n",
    "    for g_id in os.listdir(gest_folder):   #from the gest_folder\n",
    "        for i in range(1200):   #from the 1200 intial images \n",
    "            img = cv2.imread(gest_folder+\"/\"+g_id+\"/\"+str(i+1)\".jpg\",0) #Reading the images by opencv\n",
    "            if np.any(img == None):    #if any image comes to none due to threshold\n",
    "                continue\n",
    "            images_labels.append((np.array(img,dtype=np.float32),int(g_id))) #otherwise append those image in image_labels as numpy array\n",
    "    return images_lables            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_images_labels(images_labels):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for(image,label) in image_labels:\n",
    "        images.append(image) #storing all the image in image array\n",
    "        labels.append(label) #storing all the lables in labels array\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_labels = pickle_images_labels() #save the results of function into images_labels\n",
    "images_labels = shuffle(shuffle(image_labels)) #Shuffle the results of lables\n",
    "images,labels = split_images_labels(images_labels) #sliting images and lables\n",
    "print(\"Length of images labels\",len(images_labels)) #Length of image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = images[:int(5/6*len(images))] #Spliting the images into train_image from image file\n",
    "print(\"Length of train images\",len(train_images)) #Finding the lenth of train_images\n",
    "#The data format used by pickle is Python-specific. This has the advantage that there are no restrictions imposed\n",
    "#by external standards such as JSON or XDR\n",
    "#For more imformation https://docs.python.org/3/library/pickle.html#data-stream-format\n",
    "with open(\"train_images\",\"wb\") as f:  #open the train images \n",
    "    pickle.dumb(train_images,f)  #dump the images in pickle format(i.e. pythonic format)\n",
    "del train_images #delete train_image as it is of no use because we are going to use pickle one for future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = labels[:int(5/6*len(labels))] #Spliting of train_labels from the labels\n",
    "print(\"Length of train labels\",len(train_labels)) #print the length of train_labels\n",
    "with open(\"train_labels\",\"wb\") as f: #opening of file train_labels\n",
    "    pickle.dumb(train_labels,f) #dumb file of train_labels\n",
    "del train_labels #delete of train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = images[int(5/6*len(images)):] #splittig of test_images from images\n",
    "print(\"Length of test images\",len(test_images)) #length of test images\n",
    "with open(\"test_images\",\"wb\") as f: #open the test images \n",
    "    pickle.dumb(test_images,f) #dumb the test file into pickle format\n",
    "del test_images #delete of test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = labels[int(5/6*len(labels)):] #splitting of test_labels from labels\n",
    "print(\"Length of test Labels\",len(test_labels)) #length of test_labels\n",
    "with open(\"test_labels\",\"wb\") as f: #open the test_labels file\n",
    "    pickle.dumb(test_labels,f) #dumb  of the test_labels into pickle format\n",
    "del test_labels #delete of test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_size():\n",
    "    img = cv2.imread('gesture/0/100.jpg',0) #Read the images from the gven floder\n",
    "    return img.shape #give the value of total dimension i.e. nrow * ncol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gestures = os.listdir('gestures/') #list all the files in the given folder\n",
    "gestures.sort(key = int) #sort the files in ascending order\n",
    "begin_index = 0 #initialize the begin index \n",
    "end_index = 5 #initialize the end index with 5 minimum\n",
    "image_x, image_y = get_image_size() #store image into image_x & image_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(gestures)%5 !=0:           #splitting rows and the condition if number of rows not multple of 5 then calculate rows= (rows/5+1) \n",
    "    rows = int(len(gestures)/5)+1\n",
    "else:\n",
    "    rows = int(len(gestures)/5) #otherwise spliting of rows into row/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_img = None #Initialise the rows\n",
    "for i in range(rows):\n",
    "    col_img = None #Intialise the image in the given rows\n",
    "    for j in range(begin_index,end_index): #Indexing the image \n",
    "        img_path = \"gestures%s/%d.jpg\" % (j, random.randint(1,1200)) #select the image randomly from 1 to 1200\n",
    "        img = cv2.imread(img,0) #Read the images from index 0\n",
    "        if np.any(img == None): #if any image is not their\n",
    "            img = np.zeros((image_y,image_x),dtype=np.unit8) #Create empty array with imag dimension np.unit8\n",
    "        if np.any(col_img == None): #if any col_img is not their then col_img is equal to img\n",
    "            col_img = img\n",
    "        else:\n",
    "            col_img = np.hstack((col_img,img)) #Otherwise stack the image horizontally\n",
    "    begin_index += 5 #Next loop start cnsidering next 5 images\n",
    "    end_index += 5 #similarly the end index increment value 05\n",
    "    if np.any(full_img == None): #if any full_img is not there then full_img = col_img\n",
    "        full_img = col_img\n",
    "    else:\n",
    "        full_img = np.vstack((full_img,col_img)) #stacking of full_img vrtically by index of col_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.imshow('gestures',full_img) #show all the images it read\n",
    "cv2.waitkey(1) #Wait for 1 sec to next image to show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"th\" format means that the convolutional kernels will have the shape (depth, input_depth, rows, cols)\n",
    "#\"tf\" format means that the convolutional kernels will have the shape (rows, cols, input_depth, depth)\n",
    "#Therefore you can convert from the former to the later via np.transpose(x, (2, 3, 1, 0)) where x is the value of \n",
    "#the convolution kernel.\n",
    "#For more information https://stackoverflow.com/questions/39547279/loading-weights-in-th-format-when-keras-is-set-to-tf-format\n",
    "K.set_image_dim_ordering('tf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In TensorFlow 0.12+, per this issue, you can now control logging via the environmental variable called \n",
    "#TF_CPP_MIN_LOG_LEVEL; it defaults to 0 (all logs shown), but can be set to 1 to filter out INFO logs, 2 to \n",
    "#additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/0/100.jpg',0) #Reading of image and convert into array of zero and shape of (50,50)\n",
    "    return img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_of_classes():\n",
    "    return len(os.listdir('gestures/')) #number of folders or files in the given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_x,image_y = get_image_size() #seperating image_x and image_y from the images read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "    #For more information regarding model https://keras.io/layers/core/\n",
    "    num_of_classes = get_num_of_classes() # number of classes is equal to number of files in the folder\n",
    "    model = Sequential() #Calling of sequential model in keras\n",
    "    model.add(conv2D(32,(5,5),input_shape = (image_x,image_y,1),activation = 'sigmoid')) #convoluional model applied input dimension and acivation specified\n",
    "    model.add(MaxPooling2D(pool_size = (2,2),strides = (2,2),padding = 'same')) #no. of strides to be taken and paddig for corner pixels\n",
    "    #For more information https://keras.io/layers/core/#dense\n",
    "    model.add(Flatten()) #https://stackoverflow.com/questions/43237124/role-of-flatten-in-keras\n",
    "    model.add(Dense(1024,activation = 'relu')) #relu activation \n",
    "    model.add(Dropout(0.6)) #drop out all those hidden layer whic has value less than 0.6\n",
    "    model.add(Dense(num_of_classes , activation = 'softmax')) #atlast non-linear using activaton softmax\n",
    "    sgd = optimizers.SGD(lr = 1e-2) #optimized the function using Stochastic Gradient Descent and learning rate 0.01\n",
    "    model.compile(loss='categorical_crossentropy',optimizers=sgd,metrics=['accuracy'])\n",
    "    filepath = 'cnn_model_keras2.h5' #saving the model \n",
    "    checkpoint1 = ModelCheckpoint(filepath,monitor='val_acc',verbose=1,save_best_only=True,mode='max') #save the best results\n",
    "    checkpoint2 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #another level of checkpoint of saving the best result\n",
    "    callbacks_list = [checkpoint1] \n",
    "    return model,callback_list #Retrun the result of model and callbacks_list is the best variable we have selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with open(\"train_images\",\"rb\") as f: #read the train_images\n",
    "        train_images = np.array(pickle.dumb(f)) #dump the images as array in pickle format\n",
    "    with open(\"train_labels\",\"rb\") as f: #read the train_labels\n",
    "        train_labels = np.array(pickle.dumb(f),dtype = np.int32) #dump the labels as array of int 32 in pickle format\n",
    "    with open(\"test_images\",\"rb\") as f: #read the test_images\n",
    "        test_images = np.array(pickle.dumb(f)) #dump the test images in the picke format as an array \n",
    "    with open(\"test_labels\",\"rb\") as f: #read the test labels\n",
    "        test_labels = np.array(pickle.dumb(f),dtype = np.int32) #dump the test_labels in array of int32 in pickle format\n",
    "    \n",
    "    train_images = np.reshape(train_images, (train.shape[0],image_x,image_y,1)) #reshaping the train_images,so that all the images into same shape which can be ustilised in model \n",
    "    test_images  = np.reshape(test_images,(test.shape[0],image_x,image_y,1)) #reshaping the test_images\n",
    "    train_labels = np_utils.to_categorical(train_labels)  #converting tain_labels into categorical\n",
    "    test_labels = np_utils.to_categorical(test_labels) #converting test_labels into categorical\n",
    "    \n",
    "    model,callback_lists = cnn_model() #Result of above architacture uses in cnn_model() function\n",
    "     model.fit(train_images,train_labels, validation_data = (test_images,test_labels),epoch=50,batch_size=100,callbacks=callbacks_list) #training the above model architecture in train_images,train_labels \n",
    "    scores = model.evaluate(test_images,test_labels,verbose=0) #calculating the score of model \n",
    "    print(\"CNN Error: %.2f%%\" % (100- scores[1]*100)) #showing the error rate i.e. percentage of variatin not explained by model\n",
    "    model.save('cnn_model_keras.h5') #save the model results \n",
    "\n",
    "train() #start the train function by calling it seperately\n",
    "K.clear_session(); #clear the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Above explained\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) #Above explained\n",
    "classifier = tf.estimator.Estimator(model_dir=\"tmp/cnn_model2\", model_fn=cnn_model_fn) #loaidng of model by tensorflow\n",
    "prediction = None #Initialize the prediction column\n",
    "model = load_model('cnn_model_keras2.h5') #loding of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/0/100.jpg',0) #reading of image\n",
    "    return img.shape #return the image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_x,image_y = get_image_size() #Seperating the dimension into x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_image_process(img):\n",
    "    img = cv2.resize(img,(image_x,image_y)) #resizing image with specified dimension\n",
    "    img = np.array(img,dtype = n.float32) #converting the image into array with uint32 \n",
    "    np_array = np.array(img)\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_image(classifier,image):\n",
    "    global prediction #globaly used prediction for one purpose\n",
    "    processed_array = tf_process_image(image) #processing of image using above function \n",
    "    process_input_fn = tf.estimators.numpy_input_fn(x={\"x\":processed_array},shuffle = False) #seperating the variables x\n",
    "    pred = classifier.predict(input_fn = pred_input_fn) #predicting y variable from the model\n",
    "    prediction = next(pred) #The next() returns the next item from the iterator.\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_process_image(img):\n",
    "    img = cv2.resize(img,(image_x,image_y)) #resizing the image pass through keras model \n",
    "    img = np.array(img,dtype = np.float32) #converting image into arrays\n",
    "    img = np.reshape(img,(1,image_x,image_y,1)) #rehshape image into specified dimension\n",
    "    return img #return the img with new format, this is to standardise the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_predict(model,image):\n",
    "    processed = keras_process_image(image) #processed the image using above images\n",
    "    pred_probab = model.predict(processed)[0] # predict the class_labels using the keras model\n",
    "    pred_class = list(pred_probab).index(max(pred_probab)) #list the result and sort the result which has maximum probability in top\n",
    "    return max(pred_probab),pred_class # return result of max of pred_probability and the pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_text_from_db(pred_class):\n",
    "    conn = sqlite3.connect('gesture_db.db') #connect with sqllite database where we stored images \n",
    "    cmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class) #connect the pred_class with gestures_id and name \n",
    "    cursor = conn.execute(cmd) #execute the abve results\n",
    "    for row in cursor: #consider the zeroth row every element\n",
    "        return row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentence(text,num_of_words):\n",
    "    list_words = text.split(\" \") #spliting of each word with ('') used for tokenization\n",
    "    length = len(list_words) #total number of words\n",
    "    splitted_sentence = [] #Defined the empty matrice to store the splitted words of sentence\n",
    "    b_index = 0 #Initialize the index of begining\n",
    "    e_index = num_of_words #End of the index total number of words\n",
    "    while length >0: #if total number of words greater  than zero\n",
    "        part = \"\" #intialize the seperation using the part\n",
    "        for word in list_words[b_index:e_index]: #considering all the words starting from b_index to e_index\n",
    "            part = part + \" \"+ word #seperating each word(\",\")\n",
    "            splitted_sentence.append(part) #adding to sentence matrix\n",
    "            b_index += num_of_words #with each iteration b_index increases with number of words processed\n",
    "            e_index += num_of_words #with each iteration e_index increases with number of words processed\n",
    "            length -=  num_of_words #with each iteration length decreases with number of words processed\n",
    "    return splitted_sentence #return the result of all the seperated sentence into one matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def put_splitted_text_in_blackboard(blackboard,splitted_text):\n",
    "    y = 200 #intialise the value of y with 200\n",
    "    for text in splitted_text:\n",
    "        cv2.putText(blackboard,text,(4,y),cv2.FONT_HERSHEY_TRIPLEX,2,(255,255,255)) #put the text into blackboard the font is defined \n",
    "        y += 50 #It is iteration of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hand_hist():\n",
    "    with open(\"hist\",\"rb\") as f: #Read the hist file we have downloaded from kaggle famous MNIST data\n",
    "        hist = pickle.load(f) #load into the system in pickle format\n",
    "    return hist #Return the result in the above mentioned format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize():\n",
    "    global prediction  #prediction has same purpose and meaning in the entire function\n",
    "    cam = cv2.VideoCapture(1) #capture image into index 1\n",
    "    if cam.read()[0] == False: #if the image is not Caputred at index 1(it means camera at index 1)\n",
    "        cam = cv2.VideoCapture(0)#then capture the image at index 0\n",
    "    hist = get_hand_hist() #call the results from above function\n",
    "    x,y,w,h = 300,100,300,300 #defining the dimension of image\n",
    "    while True:\n",
    "        text = \"\" #intialize  the text\n",
    "        img = cam.read()[1] #Readinng of image captured\n",
    "        img = cv2.flip(img,1) #flip the image into horizontal\n",
    "        imgCrop = img[y:y+h,x:x+w] #crop the image into new dimension\n",
    "        imgHSV = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) #changing the color BGR into HSV\n",
    "        dst = cv2.calcBackProject([imgHSV],[0,1],hist,[0,180,0,256],1) #Removing the noise using BackProject\n",
    "        disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10)) #Removing the nosie \n",
    "        cv2.filter2D(dst,-1,disc,dst) #Filtering the image and converting into 2D\n",
    "        blur = cv2.GaussianBlur(dst,(11,11),0) #adding Gaussian Blur\n",
    "        blur = cv2.medianBlur(blur,15) #Adding the median Blur\n",
    "        thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1] #setting the thrshold to further process of image\n",
    "        thresh = cv2.merge(thresh,thresh,thresh) #merge all the images after applying threshold \n",
    "        thresh = cv2.cvtcolor(thresh,cv2.COLOR_BGR2GRAY) #converting the new image into GRAY\n",
    "        thresh = thresh[y:y+h,x:x+w] #setting the threshold in the dimension to standerdize\n",
    "        contours = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)[1] #finding the contours so that the all the similar points can be combined \n",
    "         if len(contours)>0: #if length of contour is greater than zero\n",
    "            contour = max(contours,key = cv2.contourArea) #take those areas which has maximum assimilation of points\n",
    "            if cv2.contourArea(contour) > 10000: #if contour Area greater than 10000\n",
    "                x1,y1,w1,h1 = cv2.boundingRect(contour) #Bound it using the defined dimension\n",
    "                save_img = thresh[y1:y1+h1,x1:x1+w1] #save image above specified dimension\n",
    "                if w1 > h1: #if width greater than height,then save image inn the below configruation\n",
    "                    save_img = cv2.copyMakeBorder(save_img,int((w1-h1)/2),int((w1-h1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "                elif h1 > w1: #if the height is greater than width,then save image into beow configruation\n",
    "                    save_img = cv2.copyMakeBorder(save_img,int((h1-w1)/2),int((h1-w1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "\n",
    "                    pred_probab,pred_class = keras_predict(model,save_img) #predict the image labels\n",
    "                    print(pred_class,pred_probab) #show the results\n",
    "                    \n",
    "                    if pred_probab * 100 > 80: # if pred_probability is greater than 80\n",
    "                        text = get_pred_text_from_db(pred_class) #Then connect with sqlite database label it associate to give gesture_id\n",
    "                        print(text) #print text\n",
    "        blackboard = np.zeros((480,640,3),dtype = np.uint8) #defining the numpy arrays with zero and given configruation\n",
    "        splitted_text = split_sentence(text,2) #split the text using split function and process it\n",
    "        put_splitted_text_in_blackboard(blackboard,splitted_text) #then display the text into blackboard using the function\n",
    "        cv2.putText(blackboard,text,(30,200),cv2.FONT_HERSHEY_TRIPLEX,1.3,(255,255,255)) #the text is processed in above function and put into specified format\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) #display of image into rectange with given dimension\n",
    "        res = np.hstack((img,blackboard)) #stack the images horizontally\n",
    "        cv2.imshow(\"Recognise gesture\",res)#show the images\n",
    "        cv2.imshow(\"thresh\",thresh) #show the images after process of threshold\n",
    "        if cv2.waitkey(1) == ord('q'): #wait for 1 sec before displaying the next image\n",
    "            break\n",
    "keras_predict(model,np.zeros(50,50),dtype = np.uint8) #predict the class labels using the keras after processing of image\n",
    "recognize() # after process of image and prediction of keras display the result by calling the given function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For more information pyttsx3 http://pyttsx3.readthedocs.io/en/latest/engine.html\n",
    "engine = pyttsx3.init() #to activate speech driver\n",
    "engine.setProperty('rate', 150)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #to avoid any type of warning or log error \n",
    "model = load_model('cnn_model_keras2.h5') #upload the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hand_hist():\n",
    "    with open(\"hist\",\"rb\") as f: #open the hist file which we have downloaded from the kaggle\n",
    "        hist = pickle.load(f) #dump the hist file into pickle format\n",
    "    return hist #return hist in the pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_size():\n",
    "    img = cv2.imread('gestures/0/100.jpg',0) #read the image and covert into the array of zero\n",
    "    return img.shape #return the shape of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_x,image_y = get_image_size() #seperate x and y dimension of zero array image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_process_image(img):\n",
    "    img = cv2.resize(img,(image_x,image_y)) #resize the image \n",
    "    img = np.array(img,dtype = uint8) #convert into array\n",
    "    img = np.reshape(img,(1,image_x,image_y,1)) #reshape the image \n",
    "    return img #return the img after the above changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_predict(model,image):\n",
    "    processed = keras_process_image(image) #processed image accoring to above condition\n",
    "    pred_probab = model.predict(processed)[0] #save the probability of image into different category\n",
    "    pred_class = list(pred_probab).index(max(pred_probab)) #list the probability accoridng to value from max to min\n",
    "    return max(pred_probab),pred_class #return max probability and pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_text_from_db(pred_class):\n",
    "    conn = sqlite3.connect('gesture_db.db') #connect to sqlite \n",
    "    cmd = \"SELECT g_name FROM gesture WHERE g_id=\"+str(pred_class) #connect the pred_clas with image g_id\n",
    "    cursor = conn.execute(cmd) #execute the command\n",
    "    for row in cursor: #consider the first row f each image\n",
    "        return row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pred_from_contour(contour,thresh):\n",
    "    x1,y1,w1,h1 = cv2.boundingRect(contour) #set boundary once the contour has finalized\n",
    "    save_img = thresh[y1:y1+h1,x1:x1+w1] #save image acccording to new specifid dimension\n",
    "    text = \"\" #intalize the text\n",
    "    if w1 > h1 : #if width is greater than height then save the image in the below mentined specification\n",
    "        save_img = cv2.copyMakeBorder(save_img,int((w1-h1)/2),int((w1-h1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "    elif h1 > w1: #if height is greater than width then save the image in the below mentioned specification\n",
    "        save_img = cv2.copyMakeBorder(save_img,int((h1-w1)/2),int((h1-w1)/2),0,0,cv2.BORDER_CONSTANT,(0,0,0))\n",
    "        pred_probab, pred_class = keras_predict(model,save_img) #new pred_probab and pred_class once image processed through keras and predict for future\n",
    "        if pred_probab * 100 > 70: #if pred_prbobaility is greater than 70 to consider\n",
    "            text = get_pred_text_from_db(pred_class) #then the text attached to pred_class processed throught given function and display result\n",
    "        return text #return the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_operator(pred_text):\n",
    "    try:\n",
    "        pred_text = int(pred_text) #converting pred_text into integer\n",
    "    except:\n",
    "        return \"\"\n",
    "    operator = \"\"\n",
    "    if pred_text == 1: #if integer comes to 1 then the operator refer to '+' similarly for others\n",
    "        operator = \"+\"\n",
    "    if pred_text == 2:\n",
    "        operator = \"-\"\n",
    "    if pred_text == 3:\n",
    "        operator = \"*\"\n",
    "    if pred_text == 4:\n",
    "        operator = \"/\"\n",
    "    if pred_text == 5:\n",
    "        operator = \"%\"\n",
    "    if pred_text == 6:\n",
    "        operator = \"**\"\n",
    "    if pred_text == 7:\n",
    "        operator = \">>\"\n",
    "    if pred_text == 8:\n",
    "        operator = \"<<\"\n",
    "    if pred_text == 9:\n",
    "        operator = \"&\"\n",
    "    if pred_text == 0:\n",
    "        operator =\"|\"\n",
    "    return operator #return the operator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = get_hand_hist() #read the image of hist\n",
    "x,y,w,h = 300,100,300,300 #setting the dimension\n",
    "is_voice_on = True #default voice is on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_img_contour_thresh(img):\n",
    "    img = cv2.flip(img,1) #flip the image horizontally\n",
    "    imgHSV = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) #convert image into bgr2hsv\n",
    "    dst = cv2.calcBackProject([imgHSV],[0,1],hist,[0,180,0,256],1) #removing the noise\n",
    "    disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(10,10))#converting to elipse to remove sharpness\n",
    "    cv2.filter2D(dst,-1,disc,dst) #converting into 2D\n",
    "    blur = cv2.GaussianBlur(dst,(11,11),0) #adding Gaussian Blur\n",
    "    blur = cv2.medainBlur(blur,15) #Adding median Blur\n",
    "    thresh = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1] #setting up thershold\n",
    "    thresh = cv2.merge(thresh,thresh,thresh) #merging the images after setting up threshold\n",
    "    thresh = cv2.cvtColor(thresh,cv2.COLOR_BGR2GRAY) #converting the threshold image into grey\n",
    "    thresh = thresh[y:y+h,x:x+w] #setting up its dimenion\n",
    "    contours = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)[1] #using contour assimilate same points\n",
    "    return img,contours,thresh   #return the results image ,contours,threshold image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def say_text(text):\n",
    "    if not is_voice_on: #if voice is not on then simply do nothing return\n",
    "        return\n",
    "    while engine._inloop:\n",
    "        pass\n",
    "    engine.say(text) # if it's on then speak the given text \n",
    "    engine.runAndWait()  #run the text in the above command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculator_mode(cam):\n",
    "    global is_voice_on   #is_voice_on has same meaning in the entire below function\n",
    "    first,operator,second = \"\",\"\",\"\" #Intialise \n",
    "    pred_text = \"\" #prediction text initialize\n",
    "    calc_text = \"\" #the text taken into consideration has initialised\n",
    "    info = \"Enter first number\" \n",
    "    #for more information https://en.wikibooks.org/wiki/Python_Programming/Threading\n",
    "    #https://www.geeksforgeeks.org/multithreading-python-set-1/\n",
    "    Thread(target = say_text,args= (info,)).start() #whatever one input it simultaeneously activate say_text function to say the text    count_clear_frames = 0\n",
    "    while True: #while the given function execute\n",
    "        img = cam.read()[1] #read the image\n",
    "        img,contours,thresh = get_img_cotour_thresh(img) #setting up the thrshold and contour\n",
    "        old_pred_text = pred_text #old_text replace and new text with change pf frame\n",
    "        if len(contours)>0: #if length of contour greater than zero\n",
    "            contour = max(contours,key = cv2.contourArea) #take those area which has maximum contour Area\n",
    "            if cv2.contourArea > 10000: #if contourArea greater than 10000\n",
    "                pred_text = get_pred_from_contour(contour,thresh) #predict the text according to the given function conditon\n",
    "                if old_pred_text == pred_text : #if previous text equal to next one then the count_same_frames increases\n",
    "                    count_same_frames += 1\n",
    "                else:\n",
    "                    count_same_frames = 0 #otherwise the frame changes\n",
    "                if pred_text == 'C': # if prediction text is 'C'\n",
    "                    if count_same_frames > 5: #if the count of same frame greater than 5\n",
    "                        count_same_frames = 0 #Initialise cont_same_frame to zero\n",
    "                        first,second,operator,pred_text,calc_text = \"\",\"\",\"\",\"\",\"\" #initialize other variables\n",
    "                        flag['first'],flag['operator'],flag['second'],flag['clear'] = False,False,False,False #take these variables false as default\n",
    "                        info ='Enter first number'\n",
    "                        Thread(target = say_text,args = (info,)).start() #then the read the new text once the new text enter\n",
    "                elif pred_text = \"Best of Luck\" and count_same_frames > 15: #if pred text is in the wrtten and number of same frame exceeds 15\n",
    "                    count_same_frame = 0 #initialize the count_same_frame to zero\n",
    "                    if flag['clear']: #if the given box is clear\n",
    "                        first,second,operator,pred_text,calc_text = \"\",\"\",\"\",\"\",\"\" #Initialize all the variables\n",
    "                        flag['first'],flag['second'],flag['operator'],flag['clear'] = False,False,False,False #taking all the variables as false\n",
    "                        info = 'Enter first number'\n",
    "                        Thread(target=say_text,args=(info,)).start() #then enter the word and simultaeneously say text\n",
    "                    elif second = \"\": #if its second\n",
    "                        flag['second'] = True \n",
    "                        info = 'clear screen' #clear the screen\n",
    "                        Thread(target = say_text,args = (info,)).start() #say the text which is going to input or predict\n",
    "                        second = \"\"\n",
    "                        flag['clear'] = True \n",
    "                        calc_text += \"= \" +str(eval(calc_text)) #evealuate the calculated text\n",
    "                        if is_voice_on: #replace the word according to processed text\n",
    "                            speech = calc_text\n",
    "                            speech = speech.replace('-','minus ')\n",
    "                            speech = speech.replace('/','divided by ')\n",
    "                            speech = speech.replace('**','raised to the power')\n",
    "                            speech = speech.replace('*','multiply')\n",
    "                            speech = speech.replace('%','mod')\n",
    "                            speech = speech.replace('>>','bitwise right shift')\n",
    "                            speech = speech.replace('<<','bitwise left shift')\n",
    "                            speech = speech.replace('&','bittwise and')\n",
    "                            speech = speech.replace('|','bitwise or')\n",
    "                        else first != '': #if first is not empty then input operator and say the text\n",
    "                            flag['first'] = True\n",
    "                            info = 'Enter operator'\n",
    "                            Thread(target = say_text,args=(info,)).start()\n",
    "                            first = ''\n",
    "                elif pred_text != \"Best of Luck \" and pred_text.isnumeric(): #if pred text is not the mentioned one and its not numeric\n",
    "                    if flag['first'] == False: #then first is false\n",
    "                        if count_same_frames > 15: #if number same frame greater than 15\n",
    "                            count_same_frame = 0 #Initialize the same frame\n",
    "                            Thread(target = say_text, args = (pred_text,)).start() #say the predicted text\n",
    "                            first += pred_text #then replace the word with pred text\n",
    "                            calc_text += pred_text #Add the pred text into caluclation text\n",
    "                    elif flag['operator'] == False: #if there is no operator\n",
    "                        operator = get_operator(pred_text) #then convert into operator\n",
    "                        if count_same_frames > 15: #if number of same frame greater than 15\n",
    "                            count_same_frames = 0 #Initialize with 0\n",
    "                            flag['operator'] = True #if there is operator\n",
    "                            calc_text += operator #add the operator into process text\n",
    "                            info = 'Enter second number' #enter the second number\n",
    "                            Thread(target = say_text,args=(info,)).start() #say the text i.e. second number\n",
    "                    elif flag['second'] == False: #if there is no second number\n",
    "                        if count_same_frame >  15: #if number of same frame greater than 15\n",
    "                            Thread(target=say_text,args=(pred_text,)).start() #say the text \n",
    "                            second += pred_text #put the predictive into second\n",
    "                            calc_text += pred_text #Add the value into processed text\n",
    "                            count_same_frame = 0 #number of same frame again intiialize to zero\n",
    "    if count_clear_frames == 30: #if number of same frame greater than 30\n",
    "            first,second,operator,pred_text,calc_text = \"\",\"\",\"\",\"\",\"\" #Intialize the variables\n",
    "            flag['first'],flag['second'],flag['operator'],flag['clear']=False,False,False,False #set the variables False\n",
    "            info = 'Enter first number' #enter the first number\n",
    "            Thread(target = say_text,args=(info,)).start() #say the text whichever is entered\n",
    "            count_clear_frames = 0 #Initialize the number of frames\n",
    "        \n",
    "        blackboard = np.zeros((480,640,3),dtype = uint8) #initialize the array\n",
    "        cv2.putText(blackboard,\"calculator mode\",(100,50),cv2.FONT_HERSHEY_TRIPLEX,1.5,(255,0,0)) #processed  text format\n",
    "        cv2.putText(blackboard,\"predicted text-\"+pred_text,(30,100),cv2.FONT_HERSHEY_TRIPLEX,1,(255,255,0)) #predicted text fprmat\n",
    "        cv2.putText(blackboard,\"operator\"+operator,(30,240),cv2.FONT_HERSHEY_TRIPLEX,1,(255,255,127)) #operator format\n",
    "        cv2.putText(blackboard,calc_text,(30,240),cv2.FONT_HERSHEY_TRIPLEX,2,(255,255,255))# calculated text format\n",
    "        cv2.putText(blackboard,info,(30,440),cv2.FONT_HERSHEY_TRIPLEX,1,(0,255,255)) # info format\n",
    "        if is_voice_on: #voice text format if on/off\n",
    "            cv2.putText(blackboard,\"Voice on\",(450,440),cv2.FONT_HERSHEY_TRIPLEX,1,(255,127,0))\n",
    "        else:\n",
    "            cv2.putText(blackboard,\"Voice off\",(450,440),cv2.FONT_HERSHEY_TRIPLEX,1,(255,127,0))\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2) #image into rectangle\n",
    "        res = np.hstack((img,blackboard))#stack horizontally\n",
    "        cv2.imshow(\"Recognizing Gesture\",res) #show the gesture\n",
    "        cv2.imshow(\"thresh\",thresh) #shoe the threshold images\n",
    "        keypress = cv2.waitkey(1) #wait for the  sec to next image t show up\n",
    "        if keypress == ord('q') or keypress == ord('t'): # if the key press q or t then come out of loop\n",
    "            break \n",
    "        if keypress == ord('v') and is_voice_on: #if the key press v then voice on/off\n",
    "            is_voice_on = False\n",
    "        elif keypress = ord('v') and not is_voice_on:\n",
    "            is_voice_on = True\n",
    "    if keypress == ord('c'):  #if the key pressed c then it give the value 2\n",
    "        return 2\n",
    "    else:\n",
    "        return 0                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_mode(cam):\n",
    "    global is_voice_on\n",
    "    text = \"\" #initialize the text\n",
    "    word = \"\" #Initialize the word\n",
    "    count_same_frame = 0 #intialize the number of same frame\n",
    "    while True:\n",
    "        img = cam.read()[1] #read the images\n",
    "        img,contours,thresh = get_img_contour_thresh(img) #get the contours and thresh image \n",
    "        old_text = text #store old text into text\n",
    "        if len(contours) > 0: #if length of contour is greater than 0\n",
    "            contour = max(contours,key = cv2.contourArea) #then consider the contour with max area\n",
    "            if cv2.contourArea > 10000: #if contour area greater than 10000\n",
    "                text = get_pred_from_contour(contourArea) #text come after the max contour area is stored into text\n",
    "                if old_text == text: #if old text is equal to text\n",
    "                    count_same_frame += 1 #Increase the count of same frame\n",
    "                else:\n",
    "                    count_same_frame = 0 #else number of same frame is zero\n",
    "                if count_same_frame > 20: #if number of same frame greater than 20\n",
    "                    if len(text) == 1: #if length of text greater than 1\n",
    "                        Thread(target = say_text,args=(text,)).start() #multiprocessing of text input \n",
    "                    word = word +text #then word is word + text\n",
    "                    if word.startswith('I/Me '): #if  word start with given word\n",
    "                        word = word.replace('I/Me ','I ') #replace it\n",
    "                    elif word.endswith('I/Me '): #else ends with specified word\n",
    "                        word = word.replace('I/Me ','me ')  #replace it accordingly\n",
    "                    count_same_frame = 0\n",
    "            elif cv2.contourArea < 10000: # if conour area less than 10000 then only say the text\n",
    "                if word != '':\n",
    "                    print = 'yolo'\n",
    "                    say_text(text)\n",
    "                    Thread(target=say_text,args=(word,)).start()\n",
    "                text = \"\"\n",
    "                word = \"\"\n",
    "        else:\n",
    "             if word != '':\n",
    "                    print('yolo1')\n",
    "                    say_text(text)\n",
    "                    Thread(target = say_text,args=(word,)).start()\n",
    "            text = \"\"\n",
    "            word = \"\"\n",
    "        blackboard = np.zeros((480,640,3),dtype = uint8)\n",
    "        cv2.putText(blackboard,\"Calculator Mode\",(100,50),cv2.FONT_HERSHEY_TRIPLEX,1.5,(255,0,0))\n",
    "        cv2.putText(blackboard,\"Predicted Text-\"+pred_text,(30,100),cv2.FONT_HERSHEY_TRIPLEX,1,(255,255,0))\n",
    "        cv2.putText(blackboard,\"operator \",+ operator,(30,140),cv2.FONT_HERSHEY_TRIPLEX,1,(255,255,127))\n",
    "        cv2.putText(blackboard,word,(30,240),cv2.FONT_HERSHEY_TRIPLEX,2,(255,255,255))\n",
    "        if is_voice_on:\n",
    "            cv2.putText(blackboard,\"Voice_on\",(450,440),cv2.FONT_HERSHEY_tRIPLEX,1,(255,127,0))\n",
    "        else:\n",
    "            cv2.putText(blackboard,\"Voice off\",(450,440),cv2.FONT_HERSHEY_TRIPLEX,1,(255,127,0))\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        res = np.hstack(\"thresh\",thresh)\n",
    "        cv2.imshow(\"Recognizing gestures\",res)\n",
    "        cv2.imshow(\"thresh\",thresh)\n",
    "        keypress = cv2.waitkey(1)\n",
    "        if keypress == ord('q') or keypress == ord('c'):\n",
    "            break\n",
    "        if keypress == ord('v') and is_voice_on:\n",
    "            is_voice_on = False\n",
    "        elif keypress == ord('v') and not is_voice_on:\n",
    "            is_voice_on = True\n",
    "    if keypress == ord('c'):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recognize():\n",
    "    cam = cv2.VideoCapture(1) #capture image from index 1\n",
    "    if cam.read()[0] == False: #if not able to capture then try index zero\n",
    "        cam = cv2.VideoCapture(0)\n",
    "    text = \"\" #initialize the text\n",
    "    word = \"\" #Initialize the word\n",
    "    count_same_frame = 0 #Initialize the number of same frame\n",
    "    keypress = 1 #Initialise the keypress\n",
    "    while True:\n",
    "        if keypress == 1: #if keypress equal to 1 then text mode starts\n",
    "            keypress = text_mode(cam)\n",
    "        elif keypress == 2: #if key press equal to 2 then calculator mode starts\n",
    "            keypress = calculator_mode(cam)\n",
    "        else:\n",
    "            break\n",
    "keras_predict(model,np.zeros((50,50),dtype = np.uint8))\n",
    "recognize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
